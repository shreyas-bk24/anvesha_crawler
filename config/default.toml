[crawler]
max_depth = 3
max_pages = 10000
concurrent_requests = 10
seed_urls = [
    "https://example.com",
    "https://httpbin.org/html"
]
user_agent = "WebCrawler/1.0"

[network]
request_timeout_secs = 30
request_delay_ms = 1000
max_retries = 3
respect_robots_txt = true
max_content_size_mb = 10
max_redirects = 10
connect_timeout_secs = 10  # This is correct
user_agents = [
    "Mozilla/5.0 (compatible; WebCrawler/1.0; +http://example.com/bot)",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
]

[storage]
database_url = "postgresql://crawler_user:crawler_pass@localhost:5432/crawler_db"
max_connections = 10
enable_caching = false
enable_foreign_keys = true  # Not used by PostgreSQL but keep for compatibility
storage_path = "./data"

[algorithms]
primary_algorithm = "bfs"
enable_opic = false
priority_boost_domains = [
    "wikipedia.org", "github.com"
]

[ranking]
relevance_weight = 0.60
pagerank_weight  = 0.25
tfidf_weight     = 0.15

